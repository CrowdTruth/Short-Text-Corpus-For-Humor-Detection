{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed succesfully\n"
     ]
    }
   ],
   "source": [
    "# Required library imports\n",
    "import csv\n",
    "import cPickle as pickle\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "import nltk.tokenize.punkt\n",
    "import nltk.data\n",
    "import string\n",
    "from collections import OrderedDict, Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "#File imports\n",
    "filepath = \"C:/Users/Sven/Google Drive/Sven vd Beukel (Msc)/datasets/\"\n",
    "proverbs = pickle.load(open(\"%sproverbs.pickle\"%filepath))\n",
    "oneliners = pickle.load(open(\"%shumorous_oneliners.pickle\"%filepath))\n",
    "headlines = pickle.load(open(\"%sreuters_headlines.pickle\"%filepath))\n",
    "wikipedia = pickle.load(open(\"%swiki_sentences.pickle\"%filepath))\n",
    "\n",
    "# Get default English stopwords and extend with punctuation\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(string.punctuation)\n",
    "stopwords.append('')\n",
    "\n",
    "# Functions\n",
    "def measure_overlap_bows(a, b, threshold=0.05): \n",
    "    \"\"\"Check if a and b are matches.\"\"\"\n",
    "    tokens_a = [token.lower().strip(string.punctuation) for token in nltk.word_tokenize(a)\\\n",
    "                if token.lower().strip(string.punctuation) not in stopwords]\n",
    "    tokens_b = [token.lower().strip(string.punctuation) for token in nltk.word_tokenize(b)\\\n",
    "                if token.lower().strip(string.punctuation) not in stopwords]\n",
    "    # Calculate Jaccard similarity\n",
    "    ratio = len(set(tokens_a).intersection(tokens_b)) / float(len(set(tokens_a).union(tokens_b)))\n",
    "    return (ratio >= threshold)\n",
    "\n",
    "def filter_similar_definitions(definition_list):\n",
    "    remove_items = []\n",
    "    for i in range(len(definition_list)+1):\n",
    "        for definition in range(i+1,len(definition_list)):\n",
    "            # Remove definitions with high path similarity\n",
    "            #similarity = definition_list[i][0].path_similarity(definition_list[definition][0])\n",
    "            #if similarity >= 0.30:\n",
    "                #print \"'%s' and '%s' have a similarity of %f\"%(definitions[k][1],definitions[s][1],similarity)\n",
    "                #remove_items.append(definition_list[definition][0])\n",
    "            # Remove items with word overlap in definitions\n",
    "            if measure_overlap_bows(definition_list[i][1], definition_list[definition][1]) == True:\n",
    "                remove_items.append(definition_list[definition][0])\n",
    "    remove_items = list(OrderedDict.fromkeys(remove_items))\n",
    "    distinct_defs = []\n",
    "\n",
    "    for item in definition_list:\n",
    "        if item[0] not in remove_items:\n",
    "            distinct_defs.append(item)\n",
    "    return distinct_defs\n",
    "\n",
    "def check_homonymy(word):\n",
    "    definitions = []\n",
    "    for i in wn.synsets(word,'n'):\n",
    "        definitions += [[i, i.definition()]]\n",
    "    definitions = filter_similar_definitions(definitions)\n",
    "    if len(definitions) >= 2:\n",
    "        return True\n",
    "    \n",
    "def split_contractions(sentence):\n",
    "    sentence = sentence.split(' ')\n",
    "    processed_words = []\n",
    "    for word in sentence:\n",
    "        if \"'s\" in word:\n",
    "            word = word.split(\"'\")\n",
    "            first_word = word[0]\n",
    "            second_word = 'is'\n",
    "            processed_words.append(first_word)\n",
    "            processed_words.append(second_word)\n",
    "        elif \"'re\" in word:\n",
    "            word = word.split(\"'\")\n",
    "            first_word = word[0]\n",
    "            second_word = 'are'\n",
    "            processed_words.append(first_word)\n",
    "            processed_words.append(second_word)\n",
    "        elif \"'ll\" in word:\n",
    "            word = word.split(\"'\")\n",
    "            first_word = word[0]\n",
    "            second_word = 'will'\n",
    "            processed_words.append(first_word)\n",
    "            processed_words.append(second_word)\n",
    "        else:\n",
    "            processed_words.append(word)\n",
    "    separator = ' '\n",
    "    processed_sentence = separator.join(processed_words)\n",
    "    return processed_sentence\n",
    "    \n",
    "def pos_tag_sentence(sentence):\n",
    "    sentence = split_contractions(sentence)\n",
    "    pos_tagger = nltk.word_tokenize(sentence)\n",
    "    sent = [word.lower().strip(string.punctuation) for word in pos_tagger]\n",
    "    tagged_sentence = pos_tag(' '.join(sent).split())\n",
    "    return tagged_sentence\n",
    "\n",
    "def find_nouns(sentence):\n",
    "    pos_tagged_sentence = pos_tag_sentence(sentence) \n",
    "    nouns = [word.lower().strip(string.punctuation) for word,pos in pos_tagged_sentence\\\n",
    "             if pos == 'NN' or pos == 'NNS']# or pos == 'NNP' or pos == 'NNPS']\n",
    "    return nouns\n",
    "\n",
    "def find_homonyms(sentence):\n",
    "    nouns = find_nouns(sentence)\n",
    "    homonym_nouns = [noun for noun in nouns if check_homonymy(noun) == True]\n",
    "    return homonym_nouns\n",
    "\n",
    "def find_homonyms_test(sentence,nouns):\n",
    "    homonym_nouns = [noun for noun in nouns if check_homonymy(noun) == True]\n",
    "    return homonym_nouns\n",
    "\n",
    "print 'Executed succesfully'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# File import:\n",
    "with open('one_liners_random_sample_nouns+homonyms.csv', 'r') as t: \n",
    "    spamreader = csv.reader(t, delimiter=';', quotechar='|')\n",
    "    sentences = []\n",
    "    annotated_nouns = []\n",
    "    annotated_homonyms = []\n",
    "    for row in spamreader:\n",
    "        sentences.append(row[0].decode('ascii', 'ignore'))\n",
    "        if len(row) >= 2:\n",
    "            annotated_nouns.append(row[1].split(','))\n",
    "        if len(row) >= 3:\n",
    "            annotated_homonyms.append(row[2].split(','))\n",
    "\n",
    "# Strip away spaces\n",
    "for i in range(len(annotated_nouns)):\n",
    "    words = []\n",
    "    for word in annotated_nouns[i]:\n",
    "        words.append(word.lower().strip())\n",
    "    annotated_nouns[i] = words\n",
    "    \n",
    "for i in range(len(annotated_homonyms)):\n",
    "    words = []\n",
    "    for h in annotated_homonyms[i]:\n",
    "        words.append(h.lower().strip())\n",
    "    words = filter(None, words)\n",
    "    annotated_homonyms[i] = words\n",
    "\n",
    "# Compare automatic recognized nouns and homonyms to annotated ones\n",
    "fp_nouns = []\n",
    "fn_nouns = []\n",
    "correct_nouns = []\n",
    "fp_homonyms = []\n",
    "fn_homonyms = []\n",
    "correct_homonyms = []\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    found_homonyms = find_homonyms_test(sentences[i],annotated_nouns[i])\n",
    "    found_nouns = find_nouns(sentences[i])\n",
    "\n",
    "    for noun in found_nouns:\n",
    "        if noun in annotated_nouns[i]:\n",
    "            correct_nouns.append(noun)\n",
    "        else:\n",
    "            fp_nouns.append(noun)\n",
    "            #print '%s was incorrectly classified as noun'%noun\n",
    "            \n",
    "    for noun in annotated_nouns[i]:\n",
    "        if noun not in found_nouns:\n",
    "            fn_nouns.append(noun)\n",
    "            #print '%s was incorrectly dismissed'%noun\n",
    "            \n",
    "    for h in found_homonyms:\n",
    "        if h in annotated_homonyms[i]:\n",
    "            correct_homonyms.append(h)\n",
    "        else:\n",
    "            fp_homonyms.append(h)\n",
    "            #print '%s was incorrectly classified as homonym'%h\n",
    "            \n",
    "    for h in annotated_homonyms[i]:\n",
    "        if h not in found_homonyms:\n",
    "            fn_homonyms.append(h)\n",
    "            #print '%s was incorrectly dismissed'%h\n",
    "\n",
    "# Calculate all evaluation measures\n",
    "# Nouns\n",
    "accuracy_nouns = 1-(1.0*(len(fp_nouns)+ len(fn_nouns))/(len(fp_nouns)+ len(fn_nouns)+ len(correct_nouns)))\n",
    "precision_nouns = 1.0 * len(correct_nouns)/ (len(correct_nouns)+len(fp_nouns))\n",
    "recall_nouns = 1.0 * len(correct_nouns)/ (len(correct_nouns)+len(fn_nouns))\n",
    "f_nouns = 2.0 * ((precision_nouns * recall_nouns) / (precision_nouns + recall_nouns))\n",
    "\n",
    "# Homonyms\n",
    "accuracy_homonyms = 1-(1.0*(len(fp_homonyms)+ len(fn_homonyms))/(len(fp_homonyms)+ len(fn_homonyms)+ len(correct_homonyms)))\n",
    "precision_homonyms = 1.0 * len(correct_homonyms)/ (len(correct_homonyms)+len(fp_homonyms))\n",
    "recall_homonyms = 1.0 * len(correct_homonyms)/ (len(correct_homonyms)+len(fn_homonyms))\n",
    "f_homonyms = 2.0 * ((precision_homonyms * recall_homonyms) / (precision_homonyms + recall_homonyms))\n",
    "\n",
    "print 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouns: \t\tFP - 45\tFN - 29\tPrecision: 0.8520\tRecall: 0.8993\tAccuracy: 0.7778\tF-measure: 0.8750\n",
      "Homonyms: \tFP - 37\tFN - 5\tPrecision: 0.7886\tRecall: 0.9650\tAccuracy: 0.7667\tF-measure: 0.8679\n"
     ]
    }
   ],
   "source": [
    "print 'Nouns: \\t\\tFP - %d\\tFN - %d\\tPrecision: %.4f\\tRecall: %.4f\\tAccuracy: %.4f\\tF-measure: %.4f'\\\n",
    "%(len(fp_nouns), len(fn_nouns),precision_nouns, recall_nouns, accuracy_nouns, f_nouns)\n",
    "print 'Homonyms: \\tFP - %d\\tFN - %d\\tPrecision: %.4f\\tRecall: %.4f\\tAccuracy: %.4f\\tF-measure: %.4f'\\\n",
    "%(len(fp_homonyms), len(fn_homonyms), precision_homonyms, recall_homonyms, accuracy_homonyms, f_homonyms)\n",
    "#print len(correct_nouns)\n",
    "#print len(correct_homonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following words were incorrectly classified as noun:\n",
      "[u'spend', u'ill', u'red', u'ex', u'fine', u'robbers', u'fancy', u'backwards', u'blonde', u'worth', u'doing', u'secret', u'fuck', u'none', u'while', u'farts', u'forget', u'wet', u'lets', u'black', u'shit', u'whatever', u'becomes', u'happens', u'ta', u'dig', u'awkward', u'awkward', u'awkward', u'counts', u'hot', u'help', u'leave', u'o', u'shea', u'stands', u'oooooh', u'better', u'random', u'brace', u'stolen', u'touches', u'say', u'pity', u'yes']\n",
      "The following nouns were incorrectly dismissed:\n",
      "['green', 'google', 'google', 'learning', 'napoleon', 'coat', 'library', 'librarian', 'half-brother', '', '%', '%', 'sweden', 'ring', 'ting', 'seed', \"o'shea\", 'cannoli', 'odeon', 'cinemas', 'funeral', 's_e_e', 'fish', 'bed', 'red', 'whisky', 'dad', 'mr.t', 'dave']\n"
     ]
    }
   ],
   "source": [
    "# Evaluate noun performance:\n",
    "print \"The following words were incorrectly classified as noun:\"\n",
    "print fp_nouns\n",
    "print \"The following nouns were incorrectly dismissed:\"\n",
    "print fn_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following words were incorrectly classified as homonym:\n",
      "['grave', 'dress', 'marriage', 'suicide', 'christmas', 'suspense', 'suicide', 'fun', 'women', 'teachers', 'problems', 'marriage', 'license', 'thought', 'women', 'interview', 'strengths', 'example', 'teacher', 'student', 'minimum', 'maximum', 'seed', 'rick', 'cinemas', 'knife', 'fight', 'clowns', 'god', 'skin', 'water', 'bone', 'gypsy', 'pushover', 'disaster']\n",
      "The following homonyms were incorrectly dismissed:\n",
      "['scotch', 'keyboard', 'hanger', 'chick', 'tiles']\n"
     ]
    }
   ],
   "source": [
    "print \"The following words were incorrectly classified as homonym:\"\n",
    "print fp_homonyms\n",
    "print \"The following homonyms were incorrectly dismissed:\"\n",
    "print fn_homonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from ark_tweet_nlp_python_master import CMUTweetTagger as CM\n",
    "#results= CM.runtagger_parse(['example tweet 1', 'example tweet 2'])\n",
    "#print results\n",
    "'''from nltk.tag import StanfordPOSTagger\n",
    "import os\n",
    "java_path = \"C:/Program Files (x86)/Java/jdk1.8.0_91\" # replace this\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "print os.environ['JAVAHOME']\n",
    "st = StanfordPOSTagger('C:/Python27/Lib/stanford-postagger-2015-12-09/models/english-bidirectional-distsim.tagger',\\\n",
    "                       'C:/Python27/Lib/stanford-postagger-2015-12-09/stanford-postagger.jar') \n",
    "st.tag('What is the airspeed of an unladen swallow ?'.split())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Archive for barely used code that may come in handy someday\n",
    "# Create Bag of Words for each dataset, ignoring most common english stopwords\n",
    "BoW_proverbs = [token.lower().strip(string.punctuation) for sentence in \\\n",
    "               proverbs for token in nltk.word_tokenize(sentence)\\\n",
    "                if token.lower().strip(string.punctuation) not in stopwords]\n",
    "\n",
    "BoW_oneliners = [token.lower().strip(string.punctuation) for sentence in \\\n",
    "               oneliners for token in nltk.word_tokenize(sentence)\\\n",
    "                if token.lower().strip(string.punctuation) not in stopwords]\n",
    "\n",
    "BoW_headlines = [token.lower().strip(string.punctuation) for sentence in \\\n",
    "               headlines for token in nltk.word_tokenize(sentence)\\\n",
    "                if token.lower().strip(string.punctuation) not in stopwords]\n",
    "# Most frequent words\n",
    "print \"The most common words in the Proverbs dataset are:\"\n",
    "print BoW_proverbs_word_freq.most_common()[:200]\n",
    "print \"The most common words in the Oneliners dataset are:\"\n",
    "print BoW_oneliners_word_freq.most_common()[:20]\n",
    "print \"The most common words in the Headlines dataset are:\"\n",
    "print BoW_headlines_word_freq.most_common()[:20]\n",
    "\n",
    "# Compute lexical diversity\n",
    "def lexical_diversity(tokens):\n",
    "    return 1.0*len(set(tokens))/len(tokens)\n",
    "\n",
    "# Compute the average number of words per tweet\n",
    "def average_words(sentences):\n",
    "    total_words = sum([len(s.split()) for s in sentences ])\n",
    "    return 1.0*total_words/len(sentences)\n",
    "\n",
    "def average_word_length(tokens):\n",
    "    total_word_length = sum([ len(w) for w in tokens ])\n",
    "    return 1.0*total_word_length/len(tokens)\n",
    "\n",
    "def tokens_per_sentence(tokens, sentences):\n",
    "    total_word_length = sum([ len(w) for w in tokens ])\n",
    "    return 1.0*total_word_length/len(sentences)\n",
    "print \"Proverbs -  Lexical Diversity: %f \\t\\t AVG Word count: %f \\t AVG Word Length: %f\"\\\n",
    "%(lexical_diversity(BoW_proverbs), average_words(proverbs), average_word_length(BoW_proverbs))\n",
    "print \"Oneliners - Lexical Diversity: %f \\t AVG Word count: %f \\t AVG Word Length: %f\"\\\n",
    "%(lexical_diversity(BoW_oneliners), average_words(oneliners), average_word_length(BoW_oneliners))\n",
    "print \"Headlines - Lexical Diversity: %f \\t AVG Word count: %f \\t AVG Word Length: %f\"\\\n",
    "%(lexical_diversity(BoW_headlines), average_words(headlines), average_word_length(BoW_headlines))\n",
    "\n",
    "print tokens_per_sentence(BoW_proverbs,proverbs)\n",
    "print tokens_per_sentence(BoW_oneliners,oneliners)\n",
    "print tokens_per_sentence(BoW_headlines,headlines)\n",
    "\n",
    "\n",
    "for word in [BoW_proverbs]:\n",
    "    BoW_proverbs_word_freq = Counter(word)\n",
    "\n",
    "for word in [BoW_oneliners]:\n",
    "    BoW_oneliners_word_freq = Counter(word)\n",
    "\n",
    "for word in [BoW_headlines]:\n",
    "    BoW_headlines_word_freq = Counter(word)\n",
    "    \n",
    "#Named Entity Recognition\n",
    "'''tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "\n",
    "def extract_entity_names(t):\n",
    "    entity_names = []\n",
    "\n",
    "    if hasattr(t, 'label') and t.label:\n",
    "        if t.label() == 'NE':\n",
    "            entity_names.append(' '.join([child[0] for child in t]))\n",
    "        else:\n",
    "            for child in t:\n",
    "                entity_names.extend(extract_entity_names(child))\n",
    "\n",
    "    return entity_names\n",
    "\n",
    "#entity_names = []\n",
    "#for tree in chunked_sentences:\n",
    "    # Print results per sentence\n",
    "    #print extract_entity_names(tree)\n",
    "\n",
    "    #entity_names.extend(extract_entity_names(tree))\n",
    "\n",
    "# Print all entity names\n",
    "#print entity_names\n",
    "\n",
    "#filepath = \"C:/Users/Sven/Google Drive/Sven vd Beukel (Msc)/datasets/\"\n",
    "#oneliners = pickle.load(open(\"%shumorous_oneliners.pickle\"%filepath))\n",
    "\n",
    "# indexpositions of random selection of oneliners, currently ununsed:\n",
    "random_selection = [75, 648, 680, 713, 735, 813, 900, 975, 984, 1062, 1156, 1226, 1302, 1372, 1415, 1491, 1578, 1661, 1663, 1680, 1832,\n",
    "             1841, 1846, 1888, 1916, 1981, 1993, 1994, 2017, 2034, 2159, 2240, 2275, 2513, 2747, 2756, 2943, 3006, 3078, 3121, \n",
    "             3162, 3184, 3187, 3242, 3267, 3505, 3510, 3551, 3620, 3626, 3643, 3649, 3843, 3853, 3858, 3925, 3927, 4002,\n",
    "             4018, 4030, 4106, 4154, 4155, 4209, 4334, 4416, 4433, 4469, 4510, 4517, 4610, 4771, 4904, 4912, 5058, 5138]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code for extracting most similar wikipedia sentences in comparison with oneliners\n",
    "unprocessed_wiki_sentences = open('wiki_sentences.txt', 'r')\n",
    "unprocessed_wiki_sentences = unprocessed_wiki_sentences.readlines()\n",
    "wiki_sents = []\n",
    "for s in unprocessed_wiki_sentences:\n",
    "    s = s.split('\\n')\n",
    "    for i in s:\n",
    "         if i == '':\n",
    "            s.remove(i)\n",
    "    wiki_sents.append(s[0])\n",
    "\n",
    "similar_sentences = []\n",
    "sentences_processed = 0\n",
    "for sentence in wiki_sents:\n",
    "    sentence = sentence.split(' ')\n",
    "    similar_words = 0\n",
    "    for word in sentence:\n",
    "        for w in BoW_proverbs_word_freq.most_common()[:200]:\n",
    "            if word.lower() == w[0]:\n",
    "                similar_words +=1\n",
    "    sentence_similarity = (1.0*similar_words)/len(sentence)\n",
    "    if sentence_similarity>0.05:\n",
    "        similar_sentences.append((sentence,sentence_similarity))\n",
    "    sentences_processed += 1\n",
    "    if sentences_processed%2000 == 0:\n",
    "        print \"processed %s sentences\"%sentences_processed\n",
    "similar_sentences2 = similar_sentences\n",
    "def getKey(item):\n",
    "    return item[1]\n",
    "similar_sentences2 = sorted(similar_sentences2, key=getKey, reverse = True)\n",
    "\n",
    "most_similar_sents = []\n",
    "for s in similar_sentences2[0:5252]:\n",
    "    most_similar_sents.append(\" \".join(s[0]))\n",
    "    \n",
    "# Store away the most similar wikipedia sentences\n",
    "f = open(\"wiki_sents.pickle\", \"wb\")\n",
    "pickle.dump(most_similar_sents, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
